//spark.sql("set hive.mapred.supports.subdirectories=true")
//spark.sql("set mapreduce.input.fileinputformat.input.dir.recursive=true")
//spark.sql("set mapred.max.split.size=256000000")
//spark.sql("set mapred.min.split.size.per.node=128000000")
//spark.sql("set mapred.min.split.size.per.rack=128000000")
//spark.sql("set hive.hadoop.supports.splittable.combineinputformat=true")
//spark.sql("set hive.exec.compress.output=true")
//spark.sql("set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec")
//spark.sql("set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat")
//
//spark.sql("set hive.merge.mapfiles=true")
//spark.sql("set hive.merge.mapredfiles=true")
//spark.sql("set hive.merge.rcfile.block.level=true")
//spark.sql("set hive.merge.size.per.task=256000000")
//spark.sql("set hive.merge.smallfiles.avgsize=256000000")
//
//spark.sql("set hive.groupby.skewindata=true")

------------->>>以上设置在spark中是没有任何卵用的<<<-------------因为底层计算是spark的，而不是MapReduce-----------------

如果没有分区的hive大表我们可以通过repartition(n)的方式来重新合并文件。以达到减少小文件的目的
实例：
spark.sql("select * from t1").repartition(1).registerTempTable("foo")
spark.sql("create table t3 select * from foo")

明天继续针对分区表小文件

create table wyp(id int, sql double, age int, s1 string, s2 string, s3 string, s4 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE;

load data local inpath '/gjx/big.txt' into table gus1